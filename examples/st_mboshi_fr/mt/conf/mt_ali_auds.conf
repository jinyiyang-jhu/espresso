# Configurations

# Directories
token_dir=exp/mb_nauds_58
exp_dir=exp/mt_ali_auds
bin_dir=${exp_dir}/bin_data

# Languages and type
type="auds"
src="mb"
tgt="fr"

# model parameters
arch="transformer"
task="translation"
share_embeddings="" # if want to use this option, add "--share-all-embeddings" in the train script.
train_num_workers=40
decode_num_workers=10
save_interval=3
ngpus=2
#keep_last_epochs=30

# encoder
encoder_layers=2
encoder_embed_dim=512
encoder_ffn_embed_dim=128
encoder_attention_heads=4

# decoder
decoder_layers=2
decoder_embed_dim=512
decoder_ffn_embed_dim=128
decoder_attention_heads=4

# training parameters
optimizer="adam"
lr_scheduler="inverse_sqrt"
update_freq=1 # gradient accumulation for every N_i batches
clip_norm=5
patience=0
dropout=0.2

max_epoch=30
lr=0.0005
init_lr=1e-5
stop_min_lr=1e-07
warmup_updates=100
weight_decay=0.0001
#length_norm_loss=
batch_size=32 # for BPE
#max_tokens=300
curriculum=0 # don't shuffle the first N epochs

criterion="label_smoothed_cross_entropy"
label_smoothing=0.1

# Transformer related
transformer_attn_dropout=0.1
#transformer_init=
